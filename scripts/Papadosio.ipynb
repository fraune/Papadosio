{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbF524qn8ga-"
      },
      "source": [
        "# Process Papadosio albums and sets from Bandcamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ6ZJxbymtts"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def parse_iso8601_duration_to_seconds(duration):\n",
        "  \"\"\"\n",
        "  Parses an ISO 8601 duration string into seconds, focusing on hours, minutes, and seconds.\n",
        "  This version is more robust and avoids errors when encountering leading zeros or empty values.\n",
        "\n",
        "  Args:\n",
        "  - duration (str): The ISO 8601 duration string.\n",
        "\n",
        "  Returns:\n",
        "  - int: The duration in seconds.\n",
        "  \"\"\"\n",
        "  # Initialize hours, minutes, and seconds to zero\n",
        "  hours = minutes = seconds = 0\n",
        "\n",
        "  # Remove the 'P' and 'T' markers\n",
        "  duration = duration.replace('P', '').replace('T', '')\n",
        "\n",
        "  # Split the duration by 'H', 'M', and 'S'\n",
        "  parts = duration.split('H')\n",
        "  if len(parts) > 1:\n",
        "    hours = int(parts[0])\n",
        "    duration = parts[1]\n",
        "\n",
        "  parts = duration.split('M')\n",
        "  if len(parts) > 1:\n",
        "    minutes = int(parts[0])\n",
        "    duration = parts[1]\n",
        "\n",
        "  parts = duration.split('S')\n",
        "  if len(parts) > 0 and parts[0]:\n",
        "    seconds = int(parts[0])\n",
        "\n",
        "  # Calculate total seconds\n",
        "  total_seconds = seconds + minutes * 60 + hours * 3600\n",
        "\n",
        "  return total_seconds\n",
        "\n",
        "\n",
        "def parse_album(album_url):\n",
        "  response = requests.get(album_url)\n",
        "  if response.status_code != 200:\n",
        "    print(f'Failed to download album page: {album_url}')\n",
        "    return {}\n",
        "\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "  script_tag = soup.find('script', type='application/ld+json')\n",
        "\n",
        "  if script_tag:\n",
        "    # Extract and parse the JSON data\n",
        "    json_data = json.loads(script_tag.string)\n",
        "  else:\n",
        "    print(\"JSON data not found\")\n",
        "    return {}\n",
        "\n",
        "  matching_release = next((release for release in json_data[\"albumRelease\"] if release[\"@id\"] == json_data[\"@id\"]), None)\n",
        "\n",
        "  if not matching_release:\n",
        "    print(\"No matching albumRelease found.\")\n",
        "    return {}\n",
        "\n",
        "  album = {\n",
        "    'name': matching_release['name'],\n",
        "    'url': json_data['mainEntityOfPage'],\n",
        "    'date_published': json_data['datePublished'],\n",
        "    'duration': 0,\n",
        "    'tracks': []\n",
        "  }\n",
        "\n",
        "  for track in json_data['track']['itemListElement']:\n",
        "    duration_seconds = parse_iso8601_duration_to_seconds(track['item']['duration'])\n",
        "    album['duration'] += duration_seconds\n",
        "    track_json = {\n",
        "      'name': track['item']['name'],\n",
        "      'duration': duration_seconds,\n",
        "      'url': track['item']['mainEntityOfPage']\n",
        "    }\n",
        "    album['tracks'].append(track_json)\n",
        "\n",
        "  return album\n",
        "\n",
        "\n",
        "BASE_URL = 'https://papadosio.bandcamp.com'\n",
        "\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(f'{BASE_URL}/music')\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "  # Get the HTML content of the page\n",
        "  html_content = response.text\n",
        "\n",
        "  # The file where you want to save the HTML content\n",
        "  file_path = 'music_page.html'\n",
        "\n",
        "  # Write the HTML content to a file\n",
        "  with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(html_content)\n",
        "\n",
        "  print(f'HTML content saved to {file_path}')\n",
        "else:\n",
        "  print('Failed to retrieve the HTML content. Status Code:', response.status_code)\n",
        "\n",
        "\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Find the 'ol' tag with id 'music-grid'\n",
        "music_grid = soup.find('ol', id='music-grid')\n",
        "\n",
        "music_data = []\n",
        "\n",
        "count = 0\n",
        "# Iterate through each 'li' in the 'ol' tag\n",
        "for li in music_grid.find_all('li'):\n",
        "  count += 1\n",
        "  print(f'Processing album #{count}')\n",
        "  # Find the 'a' tag and extract the 'href' attribute\n",
        "  a_tag = li.find('a')\n",
        "  album_link = a_tag['href']\n",
        "  # Check if the link needs to be converted to a full URL\n",
        "  if album_link.startswith('/album'):\n",
        "    album_link = BASE_URL + album_link\n",
        "  album_json = parse_album(album_link)\n",
        "  music_data.append(album_json)\n",
        "\n",
        "# print(json.dumps(music_data, indent=4))\n",
        "with open('papadosio.json', 'w', encoding='utf-8') as json_file:\n",
        "  json.dump(music_data, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dklNRkLsOrk4",
        "outputId": "e9617722-681d-4830-e817-59800da31d56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sorted, unique track names have been written to sorted_unique_track_names.txt\n"
          ]
        }
      ],
      "source": [
        "output_file_path = 'sorted_unique_track_names.txt'\n",
        "\n",
        "track_names = set()\n",
        "for album in music_data:\n",
        "  for track in album['tracks']:\n",
        "    track_name = track['name'].strip().lower() # Strip and make case-insensitive\n",
        "    track_names.add(track_name)\n",
        "\n",
        "# Sort the track names alphabetically\n",
        "sorted_track_names = sorted(track_names)\n",
        "\n",
        "# Write the sorted, unique track names to a new file\n",
        "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "    for track_name in sorted_track_names:\n",
        "        output_file.write(f'{track_name}\\n')\n",
        "\n",
        "print(f'Sorted, unique track names have been written to {output_file_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwYLEXhWUz74"
      },
      "outputs": [],
      "source": [
        "# TODO: Create search mapping file (e.g. \"2 am\" -> \"2am\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
